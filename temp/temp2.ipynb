{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using video_folder: /home/run/media/localdiskD/Ahmedabad University/6th SEM/ML/ML_2025_4_Cluster_555/dataset(Copy)/processed\n",
      "Processing directory: 10\n",
      "Found 101 normal CSV files\n",
      "Found 58 abnormal CSV files\n",
      "Processing directory: 11\n",
      "Found 248 normal CSV files\n",
      "Found 103 abnormal CSV files\n",
      "Processing directory: 12\n",
      "Found 206 normal CSV files\n",
      "Found 70 abnormal CSV files\n",
      "Loaded 786 trajectories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115528/342059682.py:64: RuntimeWarning: divide by zero encountered in divide\n",
      "  return distances / dt\n",
      "/home/run/media/localdiskD/Ahmedabad University/6th SEM/ML/ML_2025_4_Cluster_555/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:191: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/tmp/ipykernel_115528/342059682.py:64: RuntimeWarning: invalid value encountered in divide\n",
      "  return distances / dt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete. Sample data:\n",
      "   track_id  label  speed_variance  trajectory_smoothness  \\\n",
      "0         1      0        0.265442               6.992715   \n",
      "1         2      0       11.990207              16.192243   \n",
      "2         3      0        2.742187               0.217069   \n",
      "3         4      0        7.259035              13.449146   \n",
      "4         5      0       17.242700              15.200677   \n",
      "\n",
      "   direction_consistency  roundabout_traversal  path_efficiency  \n",
      "0              10.493970                     0         0.245147  \n",
      "1              87.664052                     0         0.988577  \n",
      "2              23.343119                     1         0.991076  \n",
      "3             546.752129                     1         0.993253  \n",
      "4             662.820302                     1         0.983496  \n",
      "Prepared 786 samples with 5 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/run/media/localdiskD/Ahmedabad University/6th SEM/ML/ML_2025_4_Cluster_555/.venv/lib/python3.13/site-packages/xgboost/core.py:158: UserWarning: [05:07:14] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Classification Results (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       119\n",
      "           1       0.48      0.51      0.49        39\n",
      "\n",
      "    accuracy                           0.74       158\n",
      "   macro avg       0.66      0.66      0.66       158\n",
      "weighted avg       0.75      0.74      0.74       158\n",
      "\n",
      "Accuracy: 0.74\n",
      "\n",
      "Feature Importances:\n",
      "                 Feature  Importance\n",
      "3   roundabout_traversal    0.363404\n",
      "4        path_efficiency    0.250001\n",
      "2  direction_consistency    0.136798\n",
      "0         speed_variance    0.129912\n",
      "1  trajectory_smoothness    0.119885\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data processing, clustering, and classification\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from typing import List, Tuple\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Updated Data Loading Function ---\n",
    "def load_trajectories(video_folder: str) -> Tuple[List[pd.DataFrame], List[int]]:\n",
    "    \"\"\"\n",
    "    Load trajectory data from CSV files in 'normal' and 'abnormal' subfolders within each numbered directory.\n",
    "\n",
    "    Args:\n",
    "        video_folder (str): Path to the parent folder containing numbered directories (e.g., '10', '11', '12').\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[pd.DataFrame], List[int]]: List of trajectory DataFrames and their labels (0: normal, 1: abnormal).\n",
    "    \"\"\"\n",
    "    trajectories, labels = [], []\n",
    "\n",
    "    for root, dirs, _ in os.walk(video_folder):\n",
    "        if 'normal' in dirs and 'abnormal' in dirs:\n",
    "            numbered_dir = os.path.basename(root)\n",
    "            print(f\"Processing directory: {numbered_dir}\")\n",
    "\n",
    "            # Normal trajectories\n",
    "            normal_path = os.path.join(root, 'normal')\n",
    "            normal_files = [os.path.join(normal_path, f) for f in os.listdir(normal_path) if f.endswith('.csv')]\n",
    "            print(f\"Found {len(normal_files)} normal CSV files\")\n",
    "            for file in normal_files:\n",
    "                df = pd.read_csv(file)[['frameNo', 'left', 'top', 'w', 'h']].sort_values(by='frameNo')\n",
    "                trajectories.append(df)\n",
    "                labels.append(0)\n",
    "\n",
    "            # Abnormal trajectories\n",
    "            abnormal_path = os.path.join(root, 'abnormal')\n",
    "            abnormal_files = [os.path.join(abnormal_path, f) for f in os.listdir(abnormal_path) if f.endswith('.csv')]\n",
    "            print(f\"Found {len(abnormal_files)} abnormal CSV files\")\n",
    "            for file in abnormal_files:\n",
    "                df = pd.read_csv(file)[['frameNo', 'left', 'top', 'w', 'h']].sort_values(by='frameNo')\n",
    "                trajectories.append(df)\n",
    "                labels.append(1)\n",
    "    return trajectories, labels\n",
    "\n",
    "# --- Helper Functions for Feature Extraction ---\n",
    "def df_to_points(df: pd.DataFrame) -> List[Tuple[int, float, float]]:\n",
    "    \"\"\"Convert DataFrame to list of (frame_id, x, y) points using bounding box center.\"\"\"\n",
    "    return [(int(row['frameNo']), row['left'] + row['w'] / 2, row['top'] + row['h'] / 2) for _, row in df.iterrows()]\n",
    "\n",
    "def calculate_speed(points: List[Tuple[int, float, float]], fps: float) -> np.ndarray:\n",
    "    \"\"\"Calculate speed between consecutive points.\"\"\"\n",
    "    dx = np.diff([p[1] for p in points])\n",
    "    dy = np.diff([p[2] for p in points])\n",
    "    dt = np.diff([p[0] for p in points]) / fps\n",
    "    distances = np.sqrt(dx**2 + dy**2)\n",
    "    return distances / dt\n",
    "\n",
    "def calculate_trajectory_smoothness(points: List[Tuple[int, float, float]]) -> float:\n",
    "    \"\"\"Calculate smoothness as variance of curvature (angle changes).\"\"\"\n",
    "    dx = np.diff([p[1] for p in points])\n",
    "    dy = np.diff([p[2] for p in points])\n",
    "    angles = np.arctan2(dy, dx)\n",
    "    return np.var(np.diff(angles)) if len(angles) > 1 else 0.0\n",
    "\n",
    "def calculate_direction_consistency(points: List[Tuple[int, float, float]]) -> float:\n",
    "    \"\"\"Measure total angle change to detect wrong-side or U-turns.\"\"\"\n",
    "    dx = np.diff([p[1] for p in points])\n",
    "    dy = np.diff([p[2] for p in points])\n",
    "    angles = np.arctan2(dy, dx)\n",
    "    total_change = np.sum(np.abs(np.diff(angles)))\n",
    "    return total_change if len(angles) > 1 else 0.0\n",
    "\n",
    "def derive_roundabout_geometry(normal_trajectories: List[pd.DataFrame]) -> Tuple[float, float, float]:\n",
    "    \"\"\"Derive roundabout center and radius from normal trajectories.\"\"\"\n",
    "    all_points = [p for traj_df in normal_trajectories for p in df_to_points(traj_df)]\n",
    "    if not all_points:\n",
    "        return 0.0, 0.0, 100.0  # Default values\n",
    "    points_array = np.array([(p[1], p[2]) for p in all_points])\n",
    "    center_x, center_y = np.mean(points_array, axis=0)\n",
    "    distances = np.sqrt((points_array[:, 0] - center_x)**2 + (points_array[:, 1] - center_y)**2)\n",
    "    radius = np.median(distances)\n",
    "    return center_x, center_y, radius\n",
    "\n",
    "def calculate_roundabout_traversal(points: List[Tuple[int, float, float]], center_x: float, center_y: float, radius: float) -> int:\n",
    "    \"\"\"Binary flag: 1 if trajectory passes through roundabout, 0 otherwise.\"\"\"\n",
    "    distances = np.sqrt((np.array([p[1] for p in points]) - center_x)**2 + (np.array([p[2] for p in points]) - center_y)**2)\n",
    "    return 1 if np.any(distances < radius) else 0\n",
    "\n",
    "def calculate_path_efficiency(points: List[Tuple[int, float, float]]) -> float:\n",
    "    \"\"\"Calculate efficiency as straight-line distance divided by total path length.\"\"\"\n",
    "    total_distance = np.sum(np.sqrt(np.diff([p[1] for p in points])**2 + np.diff([p[2] for p in points])**2))\n",
    "    start_end_distance = np.sqrt((points[-1][1] - points[0][1])**2 + (points[-1][2] - points[0][2])**2)\n",
    "    return start_end_distance / total_distance if total_distance > 0 else 1.0\n",
    "\n",
    "# --- Feature Extraction ---\n",
    "def extract_features(trajectories: List[pd.DataFrame], labels: List[int], fps: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract refined features from trajectories.\"\"\"\n",
    "    normal_trajectories = [traj for traj, label in zip(trajectories, labels) if label == 0]\n",
    "    center_x, center_y, radius = derive_roundabout_geometry(normal_trajectories)\n",
    "\n",
    "    results = []\n",
    "    for idx, (traj_df, label) in enumerate(zip(trajectories, labels)):\n",
    "        points = df_to_points(traj_df)\n",
    "        speeds = calculate_speed(points, fps)\n",
    "\n",
    "        features = {\n",
    "            'track_id': idx + 1,\n",
    "            'label': label,\n",
    "            'speed_variance': np.var(speeds) if len(speeds) > 0 else 0.0,\n",
    "            'trajectory_smoothness': calculate_trajectory_smoothness(points),\n",
    "            'direction_consistency': calculate_direction_consistency(points),\n",
    "            'roundabout_traversal': calculate_roundabout_traversal(points, center_x, center_y, radius),\n",
    "            'path_efficiency': calculate_path_efficiency(points),\n",
    "        }\n",
    "        results.append(features)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# --- Data Preprocessing and Cleaning ---\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean DataFrame by replacing NaN/infinite values with 0.\"\"\"\n",
    "    return df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Set path to dataset\n",
    "    video_folder = \"/home/run/media/localdiskD/Ahmedabad University/6th SEM/ML/ML_2025_4_Cluster_555/dataset(Copy)/processed\"\n",
    "    print(f\"Using video_folder: {video_folder}\")\n",
    "\n",
    "    # Load trajectories\n",
    "    trajectories, labels = load_trajectories(video_folder)\n",
    "    print(f\"Loaded {len(trajectories)} trajectories.\")\n",
    "\n",
    "    if len(trajectories) == 0:\n",
    "        print(\"No trajectories loaded. Check directory structure.\")\n",
    "    else:\n",
    "        # Extract features\n",
    "        fps = 1.0  # Adjust based on video frame rate\n",
    "        features_df = extract_features(trajectories, labels, fps)\n",
    "        print(\"Feature extraction complete. Sample data:\")\n",
    "        print(features_df.head())\n",
    "\n",
    "        # Clean data\n",
    "        cleaned_df = clean_dataframe(features_df)\n",
    "\n",
    "        # Prepare feature matrix and labels\n",
    "        numeric_features = [\n",
    "            'speed_variance', 'trajectory_smoothness', 'direction_consistency',\n",
    "            'roundabout_traversal', 'path_efficiency'\n",
    "        ]\n",
    "        X = cleaned_df[numeric_features]\n",
    "        y = cleaned_df['label']\n",
    "        print(f\"Prepared {len(X)} samples with {len(numeric_features)} features.\")\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Step 1: Apply DBSCAN for initial anomaly detection\n",
    "        db = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)\n",
    "        cluster_labels = db.labels_\n",
    "        initial_preds = [1 if label == -1 else 0 for label in cluster_labels]\n",
    "\n",
    "        # Step 2: Train XGBoost on labeled data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "        clf = xgb.XGBClassifier(random_state=42, scale_pos_weight=sum(y == 0) / sum(y == 1), use_label_encoder=False, eval_metric='logloss')\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate results\n",
    "        print(\"\\nHybrid Classification Results (XGBoost):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "        # Display feature importances\n",
    "        print(\"\\nFeature Importances:\")\n",
    "        importances = pd.DataFrame({'Feature': numeric_features, 'Importance': clf.feature_importances_})\n",
    "        print(importances.sort_values(by='Importance', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
